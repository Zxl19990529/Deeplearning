### 需要读的论文（按时间顺序）：(先看pruning和quantiation部分) <h1>
1. SBNet: Sparse Blocks Network for Fast Inference. (Pruning )  CVPR2018  
2. Extremely Low Bit Neural Network: Squeeze the Last Bit Out with ADMM(Quantization)  AAAI2018  
3. From Hashing to CNNs: Training Binary Weight Networks via Hashing(Quantization) AAAI2018  
4. A General Two-Step Quantization Approach for Low-bit Neural Networks with High Accuracy(Quantization)  CVPR2018  
5. Combined Group and Exclusive Sparsity for Deep Neural Networks. （Pruning） ICML2017  
6. Learning efficient convolutional networks through network slimming.（Pruning）  ICCV2017  
7. Channel Pruning for Accelerating Very Deep Neural Networks（Pruning）.  ICCV2017  
8. meProp: Sparsified Back Propagation for Accelerated Deep Learning with Reduced Overfitting. （Pruning） ICML2017  
9. Molchanov et al. Variational Dropout Sparsifies Deep Neural Networks.（Pruning）  ICML2017  
10. Flexpoint: An Adaptive Numerical Format for Efficient Training of Deep Neural  
Networks. (Quantization)  NIPS2017  
11. QSGD: Communication-Efficient SGD via Gradient Quantization and Encoding.(Quantization)  NIPS2017  
12. TernGrad: Ternary Gradients to Reduce Communication in Distributed Deep Learning.(Quantization)  NIPS2017  
