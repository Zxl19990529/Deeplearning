### 需要读的论文（按时间顺序）：(先看pruning和quantiation部分) <h1>
1. [SBNet: Sparse Blocks Network for Fast Inference. (Pruning )  CVPR2018](http://www.cs.toronto.edu/~mren/sbnet/papers/paper.pdf)  
2. [Extremely Low Bit Neural Network: Squeeze the Last Bit Out with ADMM(Quantization)  AAAI2018](https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/viewFile/16767/16728)
3. [From Hashing to CNNs: Training Binary Weight Networks via Hashing(Quantization) AAAI2018](https://arxiv.org/pdf/1802.02733.pdf)
4. [A General Two-Step Quantization Approach for Low-bit Neural Networks with High Accuracy(Quantization)  CVPR2018](http://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_Two-Step_Quantization_for_CVPR_2018_paper.pdf)  
5. [Combined Group and Exclusive Sparsity for Deep Neural Networks. （Pruning） ICML2017](http://proceedings.mlr.press/v70/yoon17a/yoon17a.pdf)
6. [Learning efficient convolutional networks through network slimming.（Pruning）  ICCV2017](http://openaccess.thecvf.com/content_ICCV_2017/papers/Liu_Learning_Efficient_Convolutional_ICCV_2017_paper.pdf) 
7. [Channel Pruning for Accelerating Very Deep Neural Networks（Pruning）.  ICCV2017](http://openaccess.thecvf.com/content_ICCV_2017/papers/He_Channel_Pruning_for_ICCV_2017_paper.pdf)
8. [meProp: Sparsified Back Propagation for Accelerated Deep Learning with Reduced Overfitting. （Pruning） ICML2017](https://arxiv.org/pdf/1706.06197.pdf) 
9. [Molchanov et al. Variational Dropout Sparsifies Deep Neural Networks.（Pruning）  ICML2017](https://arxiv.org/pdf/1701.05369.pdf)  
10. [Flexpoint: An Adaptive Numerical Format for Efficient Training of Deep Neural Networks. (Quantization)  NIPS2017](http://papers.nips.cc/paper/6771-flexpoint-an-adaptive-numerical-format-for-efficient-training-of-deep-neural-networks.pdf) 
11. [QSGD: Communication-Efficient SGD via Gradient Quantization and Encoding.(Quantization)  NIPS2017](http://papers.nips.cc/paper/6768-qsgd-communication-efficient-sgd-via-gradient-quantization-and-encoding.pdf) 
12. [TernGrad: Ternary Gradients to Reduce Communication in Distributed Deep Learning.(Quantization)  NIPS2017](http://papers.nips.cc/paper/6749-terngrad-ternary-gradients-to-reduce-communication-in-distributed-deep-learning.pdf)
### 对于一篇论文<h2>
- 先读懂讲啥了
- 给代码的跑一下代码
- 没给代码的试图复现代码